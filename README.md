# Fundamentals of Data Analysis Assessment

by Kenneth Linehan

## Introduction: 

I am a student studying the fundamentals of data analysis in ATU in 22/23.

This assessment will looks at some of the key topics in data analysis such as information, randomness, bias, outliers and data cleansing and analyse some of the key areas within the areas.

The information notebook will look at how we can use Python elements to manipulate data such as text. This notebook will also look at entrophy and how logarithm.

The randomness noteboook looks flipping coins and how we can use binomial distribution and it looks at concepts such as how we can get the same number of ways to get 4 tails as there to get 4 heads and also bar charts and histograms based on numpy distributions.

The bias notebook looks at cognitive bias such as only focusing on the news that confirms an opinion, but also other forms of cognitive bias and how we can have assumptions before we look at data. This notebook will also look at how working with standard deviations is greater with smaller sample sizes.

The outliers notebook looks at the morley data set which was an experiment which compares the speed of light to detect the relative motion of matter and plotting using matplotlib.In this section we will also look at the Fisher's Iris Data Set. We will finally look at Simpson's Paradox and how to plot the data.

The data cleansing notebook will look at the various ways we can use regular expressions to manipulate strings in a variety of ways such as re.search, re.match, re.findall and more.

This will also include a normal distribution notebook, which looks into some of the key concepts with visualisation of how they work. Normal distribution has been described as the most important probability distribution in statistics for many examples. I have looked at some of these examples as to how normal distribution is used in real life scenarios such as measurement of heights or rain which I have looked at by combining many different toold that Python offer such as numpy, matplotlib and Scipy.

## Table of Contents

These are direct links to each notebook:

[Information Notebook](https://github.com/KenLin765/funddata-assessment/blob/main/practicals/01-information.ipynb)

[Randomness](https://github.com/KenLin765/funddata-assessment/blob/main/practicals/02.randomness.ipynb)

[Bias](https://github.com/KenLin765/funddata-assessment/blob/main/practicals/03-Bias.ipynb)

[Outliers](https://github.com/KenLin765/funddata-assessment/blob/main/practicals/04.Outliers.ipynb)

[Data Cleansing](https://github.com/KenLin765/funddata-assessment/blob/main/practicals/05.Data-Cleansing.ipynb)


This is the link to the normal distribution notebook:

[Normal Distribution](https://github.com/KenLin765/funddata-assessment/blob/main/normal-distribution.ipynb)

## Setup

The notebooks will use Python and use libraries such as matplotlib, numpy and SciPy. I used Anaconda to develop the project on a jupyter notebook but I tested the code on Visual Code Studio and also runs in the same way.

Each notebook has the libraries needed at the start of the notebook, I would suggest running this in order to see how the code works.


## Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.



